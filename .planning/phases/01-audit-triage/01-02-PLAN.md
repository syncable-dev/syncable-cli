---
phase: 01-audit-triage
plan: 02
type: execute
---

<objective>
Create testing protocol and execute manual tool tests to document failures.

Purpose: Systematically test each agent tool to discover bugs, failures, and edge cases before Phase 9 creates automated tests.
Output: TEST-RESULTS.md documenting test outcomes for all tools.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
~/.claude/get-shit-done/references/checkpoints.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-audit-triage/TOOL-INVENTORY.md
@.planning/phases/01-audit-triage/01-01-SUMMARY.md

**Testing context:**
- Agent tools are invoked via LLM agent conversation
- Tools can be tested by prompting agent to use specific tools
- Some tools require external resources (K8s cluster, Prometheus)
- Testing should document: works/broken/partial + error messages
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create testing protocol with test cases per tool</name>
  <files>.planning/phases/01-audit-triage/TESTING-PROTOCOL.md</files>
  <action>
Using TOOL-INVENTORY.md, create testing protocol:

For each tool, define:
1. **Basic test** - Simplest invocation to verify tool runs
2. **Realistic test** - Typical usage scenario
3. **Edge case test** - Invalid input, missing resources, error paths
4. **Prerequisites** - What's needed (K8s cluster, files, etc.)

Group tests by category:
- **Always testable** - Tools that work standalone (analyze, file_ops, shell, etc.)
- **Require project** - Need codebase context (security, linting tools)
- **Require K8s** - Need cluster access (k8s_optimize, k8s_costs, k8s_drift)
- **Require Prometheus** - Need metrics server (prometheus_connect, prometheus_discover)

Include test prompts - exact phrases to ask agent to trigger each tool.

Mark test priority: Must test vs Nice to test (based on risk from inventory).
  </action>
  <verify>TESTING-PROTOCOL.md exists with test cases for all tools</verify>
  <done>Protocol covers all tools with clear test cases and prerequisites</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Testing protocol with test cases for all agent tools</what-built>
  <how-to-verify>
Execute the testing protocol:

**Setup:**
1. Run: `cargo build --release` (ensure current build)
2. Navigate to a test project directory with various file types

**Testing Process (per tool category):**

**Always Testable:**
1. Start agent: `./target/release/sync-ctl agent`
2. For each tool in category, use the test prompts from TESTING-PROTOCOL.md
3. Record: Pass/Fail/Partial, any error messages, unexpected behavior

**Require Project:**
1. Navigate to syncable-cli codebase (or another test project)
2. Start agent and test each tool
3. Record results

**Require K8s (if available):**
1. Ensure kubectl access to a cluster
2. Test k8s tools
3. If no cluster: mark as "UNTESTED - No K8s"

**Require Prometheus (if available):**
1. Ensure Prometheus endpoint accessible
2. Test prometheus tools
3. If no endpoint: mark as "UNTESTED - No Prometheus"

**For each test, record:**
- Tool name
- Test type (basic/realistic/edge)
- Result: PASS / FAIL / PARTIAL / UNTESTED
- Error message (if any)
- Notes on behavior
  </how-to-verify>
  <resume-signal>Type "testing complete" with path to your results file, or paste results directly. If tools require unavailable resources, note which were untestable.</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Compile test results into TEST-RESULTS.md</name>
  <files>.planning/phases/01-audit-triage/TEST-RESULTS.md</files>
  <action>
From human testing feedback, create structured results document:

1. **Summary table:**
   | Tool | Basic | Realistic | Edge | Status | Notes |

2. **Results by category:**
   - Working tools (all tests pass)
   - Partially working (some tests fail)
   - Broken tools (basic test fails)
   - Untested tools (missing prerequisites)

3. **Failure details:**
   For each failure:
   - Tool name
   - Test that failed
   - Error message
   - Expected vs actual behavior
   - Suspected cause (if obvious)

4. **Patterns observed:**
   - Common error types across tools
   - Tools that fail together (shared dependency?)
   - Consistent behavior issues

5. **Prerequisites needed:**
   - What resources would enable testing untested tools
   - Priority for getting those resources
  </action>
  <verify>TEST-RESULTS.md contains results for all tested tools</verify>
  <done>Results documented with clear pass/fail status and failure details</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] TESTING-PROTOCOL.md created with test cases
- [ ] Human testing completed (or tools marked untested with reason)
- [ ] TEST-RESULTS.md contains all results
- [ ] Failures documented with error messages
- [ ] Patterns noted for bug categorization
</verification>

<success_criteria>

- Testing protocol exists with reproducible test cases
- All testable tools have been tested
- Failures documented with sufficient detail for debugging
- Clear picture of what's working vs broken
</success_criteria>

<output>
After completion, create `.planning/phases/01-audit-triage/01-02-SUMMARY.md`:

# Phase 1 Plan 02: Testing Protocol & Results Summary

**[One-liner: X tools tested, Y working, Z broken]**

## Accomplishments

- Testing protocol created
- [N] tools tested manually
- [X] working, [Y] partial, [Z] broken, [W] untested

## Files Created/Modified

- `.planning/phases/01-audit-triage/TESTING-PROTOCOL.md` - Test cases per tool
- `.planning/phases/01-audit-triage/TEST-RESULTS.md` - Test outcomes

## Decisions Made

[Any decisions about what constitutes pass/fail]

## Issues Encountered

[Testing challenges, missing resources, etc.]

## Next Step

Ready for 01-03-PLAN.md (Bug Categorization & Prioritization)
</output>
